{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasMiconi/MetaMetaLearning/blob/main/MML_DMS_BS500_400T_Simplify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzGNPjUJsHzf",
        "outputId": "0535a576-3bfe-41e2-ec5e-439c0b4f4e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb  8 19:55:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYiQtLp0OCwf"
      },
      "source": [
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "334rFOcgpauJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import pdb\n",
        "\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from scipy import linalg\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from numpy import fft \n",
        "\n",
        "from scipy import io as spio\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.set_printoptions(precision=5) \n",
        "np.set_printoptions(precision=5) \n",
        "\n",
        "\n",
        "\n",
        "# Specify the test task (and its logical negation, which is also withheld from the training set)\n",
        "# TESTTASK = 'nand'; TESTTASKNEG = 'and'\n",
        "TESTTASK = 'dms'; TESTTASKNEG = 'dnms'\n",
        "\n",
        "\n",
        "LR =  .3 * 1e-2  # Adam LR. \n",
        "\n",
        "NBRUNS = 1 # 5          # Number of runs\n",
        "NBGEN = 1700 # 500      # Number of generations per run\n",
        "\n",
        "\n",
        "N = 70  # Number of neurons in the RNN.\n",
        "\n",
        "\n",
        "BS = 500 # 1000         # Batch size, i.e. population size for the evolutionary algorithm. \n",
        "assert BS % 2 == 0      # Should be even because of antithetic sampling.\n",
        "\n",
        "# Same parameters as GR Yang:\n",
        "TAU =  100  # Neuron membrane constant, in ms\n",
        "DT = 20     # Duration of a timestep, in ms\n",
        "\n",
        "\n",
        "# All the following times are in *timesteps*, not ms\n",
        "T =  50      # Number of *timesteps* per trial\n",
        "STIMTIME = 20       # Duration of stimulus input, total, *in timesteps* (not ms)\n",
        "REWARDTIME = 10     # Duration of reward signal period\n",
        "RESPONSETIME = 10   # Duration of responze period  \n",
        "STARTRESPONSETIME = 25  # Timestep  at which response period starts\n",
        "ENDRESPONSETIME = STARTRESPONSETIME + RESPONSETIME\n",
        "STARTREWARDTIME = 36    # Timsestep at which reward is deliverd and reward signal starts\n",
        "ENDREWARDTIME = STARTREWARDTIME + REWARDTIME\n",
        "assert ENDREWARDTIME < T\n",
        "\n",
        "\n",
        "JINIT = 1.5 #   Scale constant of initial network weights. See Section 2.7 in the MML paper.\n",
        "TAU_ET = 1000.0    # Time constant of the eligibility trace (in ms)\n",
        "PROBAMODUL = .1 #       Probability of receiving a random perturbation, for each neuron, at each timestep.\n",
        "ALPHAMODUL =  .5 #      Scale of the random perturbations\n",
        "ETA = .03 #             Learning rate for lifetime plasticity\n",
        "MULOSSTRACE = .9    #   Time constant for the trace of previous losses that serves as a baseline for neuromodulation\n",
        "MAXDW = 1e-2 #          Maximum delta-weight perimissible for lifetime plasticity (remember that plasticity is only applied once per trial, at reward delivery time)\n",
        "INITALPHA = .5 #        Initial alpha (plasticity parameter) value\n",
        "\n",
        "\n",
        "# The name of all the tasks\n",
        "alltasks = ['01', 'anti01', '10', 'anti10', 'watchstim1', 'watchstim2' ,'dms',  'antiwatchstim2', 'antiwatchstim1', 'or', 'and', 'nor', 'nand', 'dnms']\n",
        "\n",
        "\n",
        "NBRESPS = 1 # Only 1 possible response (\"1\") in addition to the null \"no-response\" (\"0\"), for now.\n",
        "\n",
        "\n",
        "NBSTIMNEURONS = 2   # 2 Stimulus neurons. Stimuli are binary, so both neurons receive opposite-valued inputs (or 0)\n",
        "NBREWARDNEURONS = 2 # 2 reward signal (correct/incorrect) neurons. Same as for stimulus neurons.\n",
        "NBNOISENEURONS = 1  # Noise neurons (not used in this code)\n",
        "NBBIASNEURONS = 1   # Bias neurons. Activations clamped to BIASVALUE.\n",
        "NBINPUTNEURONS = NBSTIMNEURONS + NBREWARDNEURONS +  NBNOISENEURONS + NBBIASNEURONS    # The first NBINPUTS neurons in the network are neurons (includes the bias, noise and reward inputs)\n",
        "NBRESPNEURONS = 1 + NBRESPS  # 0-resp plus all other resps (for now, only 1)\n",
        "NBAUXNEURONS = 0 #  No Auxiliary neurons for now.\n",
        "NBOUTPUTNEURONS = NBRESPNEURONS + NBAUXNEURONS      # The last NBOUTPUTNEURONS neurons in the network are output neurons\n",
        "NBRESPSIGNALNEURONS = NBRESPNEURONS     # Neurons that receive the response signal (\"what response did I just give?\")\n",
        "STIMNEURONS = np.arange(NBSTIMNEURONS)\n",
        "INPUTNEURONS = np.arange(NBINPUTNEURONS)\n",
        "OUTPUTNEURONS = np.arange(N-NBOUTPUTNEURONS, N)\n",
        "RESPNEURONS = np.arange(N-NBOUTPUTNEURONS, N-NBOUTPUTNEURONS + NBRESPNEURONS)\n",
        "assert N-NBOUTPUTNEURONS+NBRESPNEURONS == N-NBAUXNEURONS  # The last neurons are output neurons, and the last output neurons are aux neurons.\n",
        "AUXNEURONS = np.arange(N-NBAUXNEURONS, N)   # Again, no auxiliary neurons in this code.\n",
        "REWARDNEURONS = np.arange(NBSTIMNEURONS, NBSTIMNEURONS+NBREWARDNEURONS) # The neuron receiving (and broadcasting) the \"success\" signal is the one just after the stimulus inputs\n",
        "NOISENEURONS = np.arange(NBSTIMNEURONS+NBREWARDNEURONS, NBSTIMNEURONS+NBREWARDNEURONS+NBNOISENEURONS)\n",
        "BIASNEURONS = np.arange(NBSTIMNEURONS+NBREWARDNEURONS+NBNOISENEURONS, NBSTIMNEURONS+NBREWARDNEURONS+NBNOISENEURONS+NBBIASNEURONS)\n",
        "FIRSTRESPSIGNALNEURON = NBSTIMNEURONS+NBREWARDNEURONS+NBNOISENEURONS+NBBIASNEURONS   # The first neuron that receives the response signal. We'll need this later\n",
        "assert FIRSTRESPSIGNALNEURON == NBINPUTNEURONS\n",
        "RESPSIGNALNEURONS = np.arange(FIRSTRESPSIGNALNEURON, FIRSTRESPSIGNALNEURON +NBRESPSIGNALNEURONS)\n",
        "\n",
        "BIASVALUE = 1.0\n",
        "\n",
        "\n",
        "\n",
        "NBTASKSPERGEN = 2 #  2 task blocks per generation\n",
        "\n",
        "\n",
        "NBTRIALSLOSS = 100              # Evolutionary loss is evaluated over the last 100 trials of each block\n",
        "NBTRIALS =  300 + NBTRIALSLOSS  # Total number of trials per block\n",
        "# NBTRIALS =  4 + NBTRIALSLOSS \n",
        "NBNOISETRIALS = 0\n",
        "\n",
        "\n",
        "WDECAY =  3e-4 # Evolutionary weight decay parameter (for the Adam optimizer)\n",
        "L1PEN =  0 # No L1 penalty for now.\n",
        "L1PENALPHA = 0 # Same for the alpha parameters\n",
        "MUTATIONSIZE = .01 #  Std dev of the Gaussian mutations of the evolutionary algorithm\n",
        "\n",
        "REWARDSIZE = 3.0 # Size of the binary reward signal (correct/incorrect)\n",
        "STIMSIZE = 3.0 # Size of the stimulus input\n",
        "RESPSIGNALSIZE = 3.0 # Size of the response signal\n",
        "NOISESIZE = 0 # .2  # .1 # 0.1\n",
        "\n",
        "\n",
        "# Generate all the inputs and targets of each trial, for all batch individuals, for a whole task block. That is, all the inputs and \n",
        "# targets of all trials are generated together for each block. We do that to ensure proper balancing.\n",
        "def generateInputsAndTargetsForTask(mytask='watchstim1', whichresp=1):  # whichresp always 1 in this code, since NBRESPS = 1\n",
        "    assert NBOUTPUTNEURONS == 1 + NBRESPS + NBAUXNEURONS\n",
        "    assert whichresp in np.arange(1, NBRESPS+1)\n",
        "    assert NBSTIMNEURONS == 2 \n",
        "    assert mytask in alltasks\n",
        "    assert NBTRIALS > NBTRIALSLOSS\n",
        "    assert STIMTIME % 2 == 0\n",
        "\n",
        "\n",
        "    returns=[]\n",
        "    \n",
        "    # We could just use random stimuli, but we want to balance the inputs and outputs.\n",
        "\n",
        "    trials = []\n",
        "    for trialpart in range(2): # We separately balance the trials that don't count for loss, and those that do. Then we concatenate at the end.\n",
        "        nbt = NBTRIALSLOSS if  trialpart == 1 else (NBTRIALS - NBTRIALSLOSS)\n",
        "\n",
        "        # Half inputs are 1, half are 0. Also, half identical b/w both stimuli, half different.\n",
        "        stims1 = np.zeros((BS, nbt)).astype(int)\n",
        "        stims2 = np.zeros((BS, nbt)).astype(int)\n",
        "        stims1[:,:nbt//2] = 1\n",
        "        stims2[:,::2] = 1\n",
        "\n",
        "        # We need to shuffle independently in each row (i.e. for each batch element), but the same way for both stims1 and stims2 in each row\n",
        "        # Taken from StackOverflow:\n",
        "        p = np.argsort(np.random.rand(BS,nbt), axis=1)\n",
        "        stims1 = (np.array(list(map(lambda x, y: y[x], p, stims1))))\n",
        "        stims2 = (np.array(list(map(lambda x, y: y[x], p, stims2))))\n",
        "\n",
        "        trials.append((stims1, stims2))\n",
        "\n",
        "    # Now 'trials' is a list of 2 elements (non-loss trials and loss trials), each of which is a list of 2 elements (stims1, stims2), \n",
        "    # each of which is a long list of binary values (value of this particular stimulus for this particular trial).\n",
        "\n",
        "    # We rearrange them into two numpy arrays: one for stims1 and one for stims2 (each including all trials, non-loss and loss)\n",
        "    stims1 = np.concatenate((trials[0][0], trials[1][0]), axis=1)\n",
        "    stims2 = np.concatenate((trials[0][1], trials[1][1]), axis=1)\n",
        "    \n",
        "\n",
        "    # Now we have two proper lists of binary value pairs, each giving the value of one stimulus input for each trial. Let's turn this \n",
        "    # into actual numerical inputs for the network:\n",
        "    for numtrial in range(NBTRIALS):\n",
        "        \n",
        "        inpts = np.zeros((BS, NBSTIMNEURONS, STIMTIME)) # T)) # \n",
        "        \n",
        "        StimDur = STIMTIME\n",
        "\n",
        "        StartStim = 0\n",
        "\n",
        "        # We're assuming that the first two neurons are stimulus neurons\n",
        "\n",
        "        # The two stimuli are presented in succession, with both input neurons locked in opposite values to each other:\n",
        "        inpts[:, 0, StartStim:StartStim+StimDur//2 - 2] = 2.0 * stims1[:, numtrial][:, None] - 1.0\n",
        "        inpts[:, 0, StartStim+StimDur//2:StartStim+StimDur - 2] = 2.0 * stims2[:, numtrial][:, None] - 1.0\n",
        "        inpts[:, 1, StartStim:StartStim+StimDur] = -inpts[:, 0, StartStim:StartStim+StimDur] \n",
        "\n",
        "    \n",
        "        # Now we compute the targets, that is, the expected values of the output neurons, depending on inputs and tasks\n",
        "\n",
        "        # The response is binary: either you respond (i.e. only the non-null response should be active), or you don't (only the null-response neuron should be active)\n",
        "\n",
        "        # Note: For now, whichresp = NBRESPS = 1 - there is only one non-null response neuron (in addition to the null-response neuron).\n",
        "\n",
        "        assert 1+whichresp >= 2; assert 1+whichresp < 2 + NBRESPS\n",
        "        \n",
        "        inpts = torch.from_numpy(inpts).float().to(device)\n",
        "\n",
        "\n",
        "        # Generate the targets\n",
        "        tgts = np.zeros((BS, NBRESPNEURONS, RESPONSETIME))\n",
        "\n",
        "        # First we generate the expected output for the non-null response neuron, based on inputs and task:\n",
        "        if mytask == 'watchstim1':\n",
        "            tgts[:, whichresp, :] = stims1[:, numtrial][:, None]\n",
        "        elif mytask == 'watchstim2':\n",
        "            tgts[:, whichresp, :] = stims2[:, numtrial][:, None]\n",
        "        elif mytask == 'antiwatchstim1':\n",
        "            tgts[:, whichresp, :] = 1.0 - stims1[:, numtrial][:, None]\n",
        "        elif mytask == 'antiwatchstim2':\n",
        "            tgts[:, whichresp, :] = 1.0 - stims2[:, numtrial][:, None]\n",
        "        elif mytask == 'and':\n",
        "            tgts[:, whichresp, :] = (stims1[:, numtrial][:, None] * stims2[:, numtrial][:, None])\n",
        "        elif mytask == 'nand':\n",
        "            tgts[:, whichresp, :] = 1.0 - (stims1[:, numtrial][:, None] * stims2[:, numtrial][:, None])\n",
        "        elif mytask == 'or':\n",
        "            tgts[:, whichresp, :] = np.clip(stims1[:, numtrial][:, None] + stims2[:, numtrial][:, None], 0.0, 1.0)\n",
        "        elif mytask == 'nor':\n",
        "            tgts[:, whichresp, :] = 1.0 - np.clip(stims1[:, numtrial][:, None] + stims2[:, numtrial][:, None], 0.0, 1.0)\n",
        "        elif mytask == '10':\n",
        "            tgts[:, whichresp, :] = stims1[:, numtrial][:, None] * (1.0 - stims2[:, numtrial][:, None])\n",
        "        elif mytask == 'anti10':\n",
        "            tgts[:, whichresp, :] = stims1[:, numtrial][:, None] * (1.0 - stims2[:, numtrial][:, None])\n",
        "        elif mytask == '01':\n",
        "            tgts[:, whichresp, :] = (1.0 - stims1[:, numtrial][:, None]) * stims2[:, numtrial][:, None]\n",
        "        elif mytask == 'anti01':\n",
        "            tgts[:, whichresp, :] = 1.0 - (1.0 - stims1[:, numtrial][:, None]) * stims2[:, numtrial][:, None]\n",
        "        elif mytask == 'dms':\n",
        "            tgts[:, whichresp, :] = (stims1[:, numtrial][:, None] == stims2[:, numtrial][:, None])\n",
        "        elif mytask == 'dnms':\n",
        "            tgts[:, whichresp, :] = (stims1[:, numtrial][:, None] != stims2[:, numtrial][:, None])\n",
        "        else:\n",
        "            raise ValueError(\"Task not in list of allowed tasks! Task is\"+str(mytask))\n",
        "\n",
        "        assert np.all(np.logical_or(tgts == 0.0 , tgts == 1.0))\n",
        "\n",
        "        # The null-response neuron's expected output is just the opposite of the non-null response neuron output (either you respond, or you don't !)\n",
        "        tgts[:, 0, :] = 1.0 - tgts[:, whichresp, :]\n",
        "\n",
        "        tgts = torch.from_numpy(tgts).float().to(device)     \n",
        "\n",
        "        # In practice, we clip targets to 0.1/0.9 instead of actually 0.0/1.0. This may or may not help.\n",
        "        tgts.clip_(min=0.1, max=0.9)\n",
        "\n",
        "\n",
        "\n",
        "        # Finally: antithetic pairs get  identical sequences of trials !\n",
        "        inpts[BS//2:, :, :] = inpts[:BS//2, :, :]\n",
        "        tgts[BS//2:, :, :] = tgts[:BS//2, :, :]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        returns.append((inpts, tgts))\n",
        "    \n",
        "    return returns\n",
        "\n",
        "\n",
        "totalnbtasks = 0\n",
        "ticstart = time.time()\n",
        "with torch.no_grad():\n",
        "\n",
        "    # traincurves=[]\n",
        "    evolosses_allruns=[]\n",
        "    evolossesaux_allruns=[]\n",
        "    evolossesconc_allruns=[]\n",
        "    binarylosses_allruns=[]\n",
        "\n",
        "    ml0sallruns = []\n",
        "    bl0sallruns = []\n",
        "    \n",
        "    PRINTING = True # if numgen == 0 or np.random.rand() < .05 else False\n",
        "\n",
        "\n",
        "    # The code allows to have multiple runs in succession, all totally independent from each other. \n",
        "    # In practice, this is useful if you don't have too many generations per run (as with the NAND test task). \n",
        "    # For the DMS test task, we only use one run because it requires so many generations that multiple runs in the same session \n",
        "    \n",
        "    for numrun in range(NBRUNS):\n",
        "\n",
        "        # Initialize innate weights values\n",
        "        w =  torch.randn(N,N)  * JINIT / np.sqrt(N) \n",
        "        w = w.to(device)\n",
        "        \n",
        "        # Initialize alpha values - the plasticity parameters (capital-pi in the paper)\n",
        "        alpha = INITALPHA * torch.ones_like(w).to(device)\n",
        "\n",
        "        # We zero out input weights to input neurons, though it probably doesn't have any effect.\n",
        "        w.data[:NBINPUTNEURONS, :] = 0   # Each *row* of w contains the weights to a single neuron.\n",
        "        winit = w.clone()\n",
        "\n",
        "        # We will be using the Adam optimizer to apply our (hand-computed) evolutionary gradients\n",
        "        optimizer = optim.Adam([w, alpha], lr=LR, weight_decay=WDECAY)  # Default betas=(0.9, 0.999)\n",
        "\n",
        "        # Evolosses are real-valued losses used for evolution. Binarylosses are binary 'correct/wrong' signals, also used for logging.\n",
        "        evolosses = []\n",
        "        responses0 = []\n",
        "        binarylosses = []\n",
        "        wgradnorms = []\n",
        "        mytaskprev = mytaskprevprev = mytaskprevprevprev = whichrespprev = whichrespprevprev = whichrespprevprevprev = -1\n",
        "        \n",
        "\n",
        "        # Ready to start the evolutionary loop, iterating over generations (i.e. lifetimes). \n",
        "\n",
        "        for numgen in range(NBGEN):\n",
        "\n",
        "            # Every 10th generation is for testing on the withheld task (with no weight change)\n",
        "            TESTING = False\n",
        "            if numgen == 0 or numgen == NBGEN-1 or numgen % 10 == 0:\n",
        "                TESTING = True\n",
        "                if PRINTING:\n",
        "                    print(\"TESTING\")\n",
        "\n",
        "\n",
        "            tic = time.time()   \n",
        "            responses0thisgen = []\n",
        "        \n",
        "            # Generating the population of mutated individuals:\n",
        "\n",
        "            # First, batch the weights.\n",
        "            bw = torch.dstack(BS*[w]).movedim(2,0).to(device)     # batched weights\n",
        "            balpha = torch.dstack(BS*[alpha]).movedim(2,0).to(device)     # batched alphas\n",
        "            # Generate the mutations, for both w and alpha\n",
        "            # NOTE: batch element 0 (and BS/2, its antithetic pair) are NOT mutated, represent the curent unmutated candidate genotype.\n",
        "            mutations_wandalpha = []\n",
        "            for  n, x in enumerate( (bw, balpha) ):\n",
        "                mutations = torch.randn_like(x, requires_grad=False).to(device) *  MUTATIONSIZE\n",
        "                mutations[0,:,:] = 0  # 1st item in batch = current candidate\n",
        "                mutations[BS//2:, :, :] = -mutations[:BS//2, :, :]    # Antithetic sampling for mutations ! Really helps.\n",
        "                if TESTING:\n",
        "                    mutations *= 0.0  # Not strictly necessary (no weight change is applied) - results in batch score variance being caused only by randomness in trial order\n",
        "                x += mutations  \n",
        "                mutations_wandalpha.append(mutations)\n",
        "\n",
        "\n",
        "            \n",
        "            bw.data[:, :NBINPUTNEURONS, :] = 0  # Input neurons receive 0 connections. Probably not necessary.\n",
        "            bworig = bw.clone()                 # Storing the weights for comparison purposes at the gradient step (below).\n",
        "\n",
        "            responsecounts = torch.zeros(BS, NBRESPS + 1).to(device)\n",
        "            lifelosses = torch.zeros(BS, requires_grad=False).to(device)\n",
        "            lifemselosses = torch.zeros(BS, requires_grad=False).to(device)\n",
        "            # lifemselossesaux = torch.zeros(BS, requires_grad=False).to(device)\n",
        "            lifeblosses = torch.zeros(BS, requires_grad=False).to(device)\n",
        "                \n",
        "            allresps=[]\n",
        "\n",
        "\n",
        "            # Lifetime loop, iterates over task-blocks:\n",
        "            for numtask in range(NBTASKSPERGEN):\n",
        "                totalnbtasks += 1\n",
        "\n",
        "                # bpw = batched plastic weights\n",
        "                bpw = torch.zeros_like(bw).to(device)  # For now, plastic weights are initialized to 0 at the beginning of each task.\n",
        "\n",
        "                # Initialize neural states\n",
        "                bstates = .1 * torch.ones(BS, N).to(device)  # bstates (batched states) contains the neural activations (before nonlinearity). Dimensionality appropriate for batched matrix multiplication. \n",
        "                bstates[:, INPUTNEURONS] = 0\n",
        "                bresps = 1.0 * bstates  # bresps is the actual neural responses, after nonlinearity, and also serve as the input for the next step.\n",
        "                bresps[:, BIASNEURONS] = BIASVALUE\n",
        "\n",
        "                meanlosstrace = torch.zeros(BS, 2 * 2).to(device)\n",
        "                bls = []    # Will store binary losses of all batch elements, for each trial of this task\n",
        "                bl0s = []   # Same but only for batch element 0 (i.e. the unmutated candidate genome)\n",
        "                ml0s = []   # MSE loss \n",
        "\n",
        "\n",
        "                # Choose the task ! If not testing, makes sure it's different from recently chosen tasks.\n",
        "\n",
        "                if TESTING: \n",
        "                    mytask = TESTTASK \n",
        "                    mytasknum = alltasks.index(mytask)\n",
        "                    whichresp = 1   # This is always 1 for now, because NBRESP=1, i.e. we only use one response output (other than the null, \"no-response\" output)\n",
        "                else:\n",
        "                    while True:\n",
        "                        mytasknum = np.random.randint(len(alltasks))\n",
        "\n",
        "                        mytask = alltasks[mytasknum]\n",
        "                        whichresp = 1 \n",
        "\n",
        "                        if ( (mytask!= TESTTASK)  \n",
        "                            and (mytask != TESTTASKNEG)  # We withhold both the test task and its logical negation\n",
        "                            and     (mytask != mytaskprev) \n",
        "                                and (mytask != mytaskprevprev) \n",
        "                        ):\n",
        "\n",
        "                            break\n",
        "\n",
        "                    mytaskprevprev = mytaskprev; mytaskprev= mytask\n",
        "                    whichrespprevprev = whichrespprev; whichrespprev = whichresp\n",
        "                \n",
        "                # Cumulative MSE and binary losses for this task, over the last NBLOSSTRIALS of the block:\n",
        "                taskmselosses = torch.zeros_like(lifemselosses).to(device)\n",
        "                taskblosses = torch.zeros_like(lifemselosses).to(device)\n",
        "\n",
        "                if PRINTING:\n",
        "                    print(\"task\", mytask, \"whichresp:\", whichresp)\n",
        "                \n",
        "                # OK, ready to start the task.\n",
        "\n",
        "                # Generate the task data (inputs and targets) for all trials:\n",
        "                taskdata = generateInputsAndTargetsForTask(mytask=mytask, whichresp=whichresp)\n",
        "\n",
        "                eligtraces =   torch.zeros_like(bw, requires_grad=False).to(device)  # Initialize the eligibility traces at the start of each block/task.\n",
        "\n",
        "                # Task loop, iterating over trials\n",
        "                # You do NOT erase memory (neural activations or plastic weights) between successive trials ! \n",
        "                for numtrial in range(NBTRIALS):\n",
        "                    \n",
        "                    # Initializations\n",
        "                    losses = torch.zeros(BS, requires_grad=False).to(device)     # Losses for this trial\n",
        "                    mselossesthistrial = torch.zeros(BS, requires_grad=False).to(device)     # Losses for this trial\n",
        "                    totalouts = torch.zeros(BS, NBOUTPUTNEURONS, requires_grad=False).to(device)  \n",
        "\n",
        "                    # Extract the inputs and targets for this trial:\n",
        "                    inputs, targets = taskdata[numtrial]\n",
        "                    assert inputs.shape == (BS, NBSTIMNEURONS, STIMTIME)\n",
        "\n",
        "                    # Run the network. Trial loop, iterating over timesteps\n",
        "                    for numstep in range(T):          \n",
        "\n",
        "                        # Update neural activations\n",
        "                        bstates += (DT / TAU) * (-bstates +  torch.bmm((bw + balpha * bpw), bresps[:, :, None])[:,:,0] )  \n",
        "\n",
        "\n",
        "                        # Applying the random perturbations on neural activations, a.k.a. \"modulations\", both for noise and for the lifetime plasticity algorithm (node-perturbation)\n",
        "                        # Ans also updating the eligibility trace appropriately\n",
        "                        if numstep > 1 : \n",
        "                            modulindices =  (torch.rand(1, N) < PROBAMODUL).int()   # Which neurons get perturbed?\n",
        "                            modulations = (ALPHAMODUL * modulindices * (2 * torch.rand(1, N) - 1.0)).to(device)  # Note the dimensions: the same noise vector is applied to all elements in the batch (to save time!)\n",
        "                           \n",
        "                            bstates += modulations\n",
        "                            \n",
        "                            # Node-perturbation: Hebbian eligibility trace = product between inputs (bresps from previous time step) and *perturbations* in outputs. dH = X * deltaY \n",
        "                            # We do this with a (batched) outer product between the (column) vector of modulations (1 per neuron) and the (row) vector of inputs\n",
        "                            # Note that here, since we have an RNN, the input is bresps - the network's responses from the previous time step\n",
        "                            if torch.sum(modulindices) > 0:\n",
        "                                eligtraces += torch.bmm( modulations.expand(BS, -1)[:, :, None],  bresps[:, None, :] ) \n",
        "\n",
        "                        # Eligibility traces, unlike actual plastic weights, are decaying\n",
        "                        eligtraces -=  (DT / TAU_ET) * eligtraces\n",
        "\n",
        "\n",
        "                        # We can now compute the actual neural responses for this time step, applying the appropriate nonlinearity to each neuron\n",
        "                        bresps = bstates.clone() # F.leaky_relu(bstates)\n",
        "                        # The following assumes that output neurons are the last neurons of the network !                        \n",
        "                        bresps[:,N-NBOUTPUTNEURONS:].sigmoid_()     # The output neurons are sigmoids, all others are tanh. An arbitrary design choice.\n",
        "                        bresps[:,:N-NBOUTPUTNEURONS].tanh_()\n",
        "                        \n",
        "\n",
        "                        torch.clip_(bresps, min=-5, max=5)  # Unnecessary if all nonlinearities are bounded.\n",
        "\n",
        "\n",
        "                        # Are we in the input presentation period? Then apply the inputs.\n",
        "                        # Inputs are clamping, fixing the response of the input neurons.\n",
        "                        if numstep < STIMTIME:\n",
        "                            bresps[:, STIMNEURONS] = STIMSIZE * inputs[:, :, numstep]        \n",
        "                        else:\n",
        "                            bresps[:, STIMNEURONS] = 0\n",
        "\n",
        "                        bresps[:, BIASNEURONS] = BIASVALUE\n",
        "\n",
        "\n",
        "                        # Are we in the response period? Then collect network response.\n",
        "                        if numstep >= STARTRESPONSETIME and numstep < ENDRESPONSETIME:\n",
        "\n",
        "                            assert numstep < STARTREWARDTIME\n",
        "                            # Accumulate the total activation of each output neuron, so that we can compute the network's actual response at the end of response period:\n",
        "                            totalouts +=  bresps[:, OUTPUTNEURONS] \n",
        "                            # Accumulate the MSE error between actual and expected outputs:\n",
        "                            mselossesthistrial += torch.sum( (bresps[:, RESPNEURONS] - targets[:, :, numstep - STARTRESPONSETIME]) ** 2, axis=1 ) / RESPONSETIME\n",
        "\n",
        "                        else:\n",
        "                            bresps[:, OUTPUTNEURONS] = 0.0\n",
        "\n",
        "\n",
        "                        # Is the response period finished, or equivalently, are we at the first step of the reward / feedback period?\n",
        "                        # If so, compute the network's response and apply plasticity (i.e. deliver the neuromodulation).\n",
        "                        if numstep == STARTREWARDTIME:\n",
        "                            # The network's response for this trial (0 or 1) is the index of the output neuron that had the highest cumulative output over the response period\n",
        "                            totalresps = totalouts[:, :NBRESPNEURONS]   # Assumes that the response neurons are the first among output neurons !\n",
        "                            responses = torch.argmax(totalresps, dim=1)  # responses is a 1D, integer-valued array of size BS. totalresps is a 2D real-vlued array of size BS, NBRESPS+1                           \n",
        "                            \n",
        "                            # blosses (binary losses) is a 1/-1 \"correct/wrong\" signal for each batch element for this trial.\n",
        "                            blosses = 2.0 * (responses == torch.argmax(targets[:, :, 0], dim=1)).float() - 1.0    \n",
        "                            responses0thisgen.append(float(responses[0]))\n",
        "\n",
        "                            # We also want the 1-hot version of the response for each neuron. This will be used as the response signal below.\n",
        "                            if numtrial > 0:\n",
        "                                responses1hot_prev = responses1hot.clone()\n",
        "                            responses1hot = F.one_hot(responses, 1+NBRESPS)\n",
        "                            if numtrial < NBTRIALS - NBTRIALSLOSS:\n",
        "                                responsecounts += responses1hot\n",
        "\n",
        "                            # Now we apply lifetime plasticity, with node-perturbation, based on eligibility trace and suitably baselined reward/loss\n",
        "\n",
        "                            # We compute separate baseline (running average) losses for different types of trials, as defined by their inputs (as in Miconi, eLife 2017). \n",
        "                            # So we need to find out the trial type for each element in batch.\n",
        "                            input1 = inputs[:, 0, 0]; input2 = inputs[:, 1, 0]  # This assumes that inputs are constant and start from timestep 0 !\n",
        "                            trialtypes = (input1>0).long() * 2 + (input2>0).long()\n",
        "\n",
        "                            # Compute and apply the plasticity, based on accumulated eligibility traces and reward                          \n",
        "                            if numtrial > 30:  # Lifetime plasticity is only applied after a few burn-in trials.\n",
        "                                # eligtraces: BS x N x N.   mselossesthhistrial:  BS.    meanlosstrace: BS x (N.N).    trialtypes: BS\n",
        "                                # dw should have shape BS x N x N, i.e. one for each connection and batch element. Do not sum over batch dimension! The batch is purely evolutionary !\n",
        "\n",
        "\n",
        "                                dw =   (ETA * eligtraces  * (  meanlosstrace[np.arange(BS), trialtypes] * (mselossesthistrial - meanlosstrace[np.arange(BS), trialtypes]) )[:, None, None]).clamp(-MAXDW, MAXDW)\n",
        "                                # dw =   (ETA * eligtraces  * (  meanlosstrace[np.arange(BS), trialtypes] * (mselossesthistrial ) )[:, None, None]).clamp(-MAXDW, MAXDW)\n",
        "\n",
        "\n",
        "                                bpw -= dw\n",
        "\n",
        "                                if PRINTING and np.random.rand() < .02:\n",
        "                                    print(numrun, numtrial, \"{:.4f}\".format(float(torch.mean(mselossesthistrial))), \"{:.4f}\".format(float(torch.norm(bw+bpw))), \n",
        "                                                                        \"{:.4f}\".format(float(torch.norm(dw))), \"{:.4f}\".format(float(torch.norm(bpw))))\n",
        "                            # Updating the baseline - running average of losses, for each batch element, for the trial type just seen\n",
        "                            meanlosstrace[torch.arange(BS).long(), trialtypes] *= MULOSSTRACE\n",
        "                            meanlosstrace[torch.arange(BS).long(), trialtypes] +=  (1.0 - MULOSSTRACE) * mselossesthistrial\n",
        "\n",
        "\n",
        "\n",
        "                        # Are we in the reward signal period?\n",
        "                        # The neuromodulatory reward signal is applied just once per trial, above. Here we provide a binary \"correct/ incorrect\" signal to the network, \n",
        "                        # i.e. \"was my response right or wrong for this trial?\" \n",
        "                        # We also provide a signal indicating which response it gave in this trial (in theory it should be able to calculate it itself if needed, but this may help)\n",
        "                        if numstep >= STARTREWARDTIME and numstep < ENDREWARDTIME: # Note that by this time, the loss has been computed and is fixed\n",
        "                            \n",
        "                            # We provide a binary, \"correct/incorrect\" signal to the network\n",
        "                            bresps[:,REWARDNEURONS[0]] = REWARDSIZE * blosses[:]         # Reward input is also clamping\n",
        "                            bresps[:,REWARDNEURONS[1]] = -REWARDSIZE * blosses[:]         # Reward input is also clamping\n",
        "\n",
        "                            bresps[:,REWARDNEURONS].clip_(min=0)\n",
        "\n",
        "                            # We provide the network with a signal indicating the actual response it chose for this trial.\n",
        "                            # bresps[:, RESPSIGNALNEURONS] = responses1hot.float()\n",
        "                            # # The following is probably unnecessarily obscure and may be replaced with simply putting in the one-hot version of responses multiplied by Rewardsize:\n",
        "                            bresps[:, RESPSIGNALNEURONS] = 0\n",
        "                            bresps.scatter_(1, FIRSTRESPSIGNALNEURON + responses[:, None],   RESPSIGNALSIZE)  # The ResponseSignalNeuron corresponding to the actual response gets value RESPSIGNALSIZE - all others get 0\n",
        "\n",
        "                            \n",
        "                        else:\n",
        "                            bresps[:,REWARDNEURONS] = 0\n",
        "                            bresps[:, RESPSIGNALNEURONS] = 0\n",
        "\n",
        "\n",
        "                   \n",
        "                    # Now all steps done for this trial:\n",
        "                \n",
        "                    if PRINTING:\n",
        "                        if np.random.rand() < .1: \n",
        "                            print(\"|\", int(responses[0]), int(blosses[0]), end=' ')\n",
        "                    \n",
        "                    ml0s.append(float(mselossesthistrial[0]))\n",
        "                    bl0s.append(float(blosses[0]))\n",
        "                    bls.append(blosses.cpu().numpy())\n",
        " \n",
        "\n",
        "                    # If this trial is part of the last NBTRIALSLOSS, we accumulate its trial loss into the agent's total loss for this task.\n",
        "                    if numtrial >= NBTRIALS - NBTRIALSLOSS:     # Lifetime  losses are only estimated over the last few trials\n",
        "                        taskmselosses += 2 * mselossesthistrial / NBTRIALSLOSS   # the 2* doesn't mean anything\n",
        "                        taskblosses += blosses / NBTRIALSLOSS\n",
        "                \n",
        "\n",
        "                # Now all trials done for this task:\n",
        "                if PRINTING:\n",
        "                    print(\"Med task mseloss:\", \"{:.4f}\".format(float(torch.median(taskmselosses))))\n",
        "\n",
        "\n",
        "                    # print(\"\")\n",
        "                lifemselosses += taskmselosses / NBTASKSPERGEN \n",
        "                lifeblosses += taskblosses / NBTASKSPERGEN \n",
        "            \n",
        "                if (TESTING or numgen == 0) and numtask == 0:\n",
        "                    # These files contain respectively the first and *latest* Testing block of the *current* run only. \n",
        "                    FNAME = 'bl1block_gen0.txt' if numgen == 0 else 'bl1block_lastgen.txt'\n",
        "                    # np.savetxt(FNAME, np.array(bl0s))\n",
        "                    np.savetxt(FNAME, np.vstack(bls))\n",
        "                if numtask == 0 and (numgen == 0 or numgen == NBGEN-1):\n",
        "                    # We append the 1st and last block of each run, all together alternating in the same array\n",
        "                    assert (TESTING)\n",
        "                    ml0sallruns.append(ml0s)\n",
        "                    bl0sallruns.append(bl0s)\n",
        "\n",
        "\n",
        "            # After all tasks done for this lifetime / generation:\n",
        "\n",
        "            # if (numgen+1) == NBGEN // 3 or (numgen+1) == 2 * NBGEN // 3:\n",
        "            #     for g in optimizer.param_groups:\n",
        "            #         # g['lr'] /= 4.0 \n",
        "            #         g['lr'] /= 10.0 \n",
        "            #         g['weight_decay'] /= 10.0\n",
        "\n",
        "\n",
        "            # responseprobas = responsecounts /  torch.sum(responsecounts, dim=1, keepdim=True)\n",
        "            # responseprobassq = responseprobas * responseprobas\n",
        "            # concentrations = torch.sum(responseprobassq, dim=1)\n",
        "\n",
        "            lifelosses = lifemselosses \n",
        "\n",
        "            binarylosses.append(float(lifeblosses[0]))\n",
        "            evolosses.append(float(lifemselosses[0]))\n",
        "\n",
        "            np.savetxt('blosses_onerun.txt', np.array(binarylosses))\n",
        "            np.savetxt('mselosses_onerun.txt', np.array(evolosses))\n",
        "            \n",
        "\n",
        "            # Now we're ready to perform evolution (by computing gradients by hand, and then applying the optimizer with these gradients)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Gradient is just loss x mutation (remember we use antithetic sampling)\n",
        "            gradient = torch.sum(mutations_wandalpha[0] * lifelosses[:, None, None], axis=0) # / BS\n",
        "            wgradnorm = float(torch.norm(gradient))\n",
        "            wgradnorms.append(wgradnorm)\n",
        "            if PRINTING:\n",
        "                print(\"norm w:\", \"{:.4f}\".format(float(torch.norm(w))), \"norm gradient:\", \"{:.4f}\".format(wgradnorm), \n",
        "                        \"norm a:\", \"{:.4f}\".format(float(torch.norm(alpha))), \"mean a:\",  \"{:.4f}\".format(float(torch.mean(alpha))))\n",
        "\n",
        "\n",
        "            w.grad = gradient\n",
        "            wprev = w.clone()\n",
        "\n",
        "            gradientalpha = torch.sum(mutations_wandalpha[1] * lifelosses[:, None, None], axis=0) # / BS\n",
        "            alpha.grad = gradientalpha\n",
        "            alphaprev = alpha.clone()\n",
        "\n",
        "            if numgen > 0 and not TESTING:\n",
        "                optimizer.step()\n",
        "                w -= L1PEN * torch.sign(w)\n",
        "                alpha -= L1PENALPHA * torch.sign(alpha)\n",
        "            \n",
        "            wdiff = w - wprev\n",
        "            adiff = alpha - alphaprev\n",
        "            if PRINTING:\n",
        "                print(\"Norm w-wprev:\", \"{:.4f}\".format(float(torch.norm(wdiff))), \"Max abs w-wprev:\", \"{:.4f}\".format(float(torch.max(torch.abs(wdiff)))), \n",
        "                    \"Norm a-aprev:\", \"{:.4f}\".format(float(torch.norm(adiff))), \"Max abs a-aprev:\", \"{:.4f}\".format(float(torch.max(torch.abs(adiff))))  )\n",
        "\n",
        "        \n",
        "\n",
        "            if PRINTING:\n",
        "                print(\"Med/min/max/Half-Nth/0th loss in batch:\", float(torch.median(lifelosses)), float(torch.min(lifelosses)), float(torch.max(lifelosses)),\n",
        "                                        float(lifelosses[BS//2]), float(lifelosses[0]))\n",
        "                print(\"Gen\", numgen, \"of run\", numrun, \"done in\", time.time()-tic)\n",
        "        \n",
        "\n",
        "\n",
        "        # After all generations for this run are done:\n",
        "\n",
        "        # These arrays contain the first and last block (both on test task) of each run to date, in alternation\n",
        "        mlnp = np.vstack(ml0sallruns)\n",
        "        np.savetxt('meanlosses0_1stLastB_allruns.txt', mlnp)\n",
        "        blnp = np.vstack(bl0sallruns)\n",
        "        np.savetxt('blosses0_1stLastB_allruns.txt', blnp)\n",
        "\n",
        "        binarylosses_allruns.append(binarylosses)\n",
        "        evolosses_allruns.append(evolosses)\n",
        "\n",
        "        with open('binlossDMS_LR'+str(LR)+'MutSize'+str(MUTATIONSIZE)+'NoiseS'+str(NOISESIZE)+'_RSize'+str(REWARDSIZE)+'SSize'+str(STIMSIZE)+'.txt', 'w') as myfile:\n",
        "            for c in binarylosses_allruns:\n",
        "                for l in c:\n",
        "                        myfile.write(str(l)+\" \")\n",
        "                myfile.write(\"\\n\")\n",
        "        with open('evolossDMS_LR'+str(LR)+'MutSize'+str(MUTATIONSIZE)+'NoiseS'+str(NOISESIZE)+'_RSize'+str(REWARDSIZE)+'SSize'+str(STIMSIZE)+'.txt', 'w') as myfile:\n",
        "            for c in evolosses_allruns:\n",
        "                for l in c:\n",
        "                        myfile.write(str(l)+\" \")\n",
        "                myfile.write(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Time taken:\", time.time()-ticstart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU1HPtJljb4w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt; import numpy as np\n",
        "\n",
        "# Showing the success rate over the batch (i.e. the mean binary loss over the batch) for each trial of the last test-task block.\n",
        "\n",
        "z = .5 + .5 * np.vstack(bls)\n",
        "print(z.shape)\n",
        "plt.plot(np.mean(z, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# % correct over last 100 trials of each generation (unmutated candidate genotype only), shown separately for the non-test tasks (blue) and test task (red)\n",
        "# Median and IQR over all runs. \n",
        "\n",
        "bl = .5 + .5 * np.loadtxt('binlossDMS_LR0.003MutSize0.01NoiseS0_RSize3.0SSize3.0.txt')\n",
        "if(len(bl.shape)<2):   # If there is only a single run, add a singleton dimension\n",
        "    bl = bl[None, :]\n",
        "print(bl.shape)\n",
        "ss = bl.shape[1]  # Number of generations\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "\n",
        "xr = np.arange(len(bl[0,:]))\n",
        "plt.fill_between(xr[xr%10 != 0], np.quantile(bl, .25, axis=0).T[xr % 10 != 0], np.quantile(bl, .75, axis=0).T[xr % 10 != 0], color='b', alpha=.3)\n",
        "plt.plot(xr[xr % 10 != 0], np.quantile(bl, .5, axis=0).T[xr % 10 != 0], 'b', label='Training tasks');\n",
        "plt.fill_between(xr[::10], np.quantile(bl, .25, axis=0).T[0::10], np.quantile(bl, .75, axis=0).T[0::10], color='r', alpha=.3)\n",
        "plt.plot(xr[::10], np.quantile(bl, .5, axis=0).T[0::10], 'r', label='Test task')\n",
        "\n",
        "\n",
        "plt.xlabel('Generations')\n",
        "plt.ylabel('% correct over last 100 trials')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.title('Test task: '+str(TESTTASK).upper())\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('evoloss'+str(TESTTASK).upper()+'.png', dpi=300)\n"
      ],
      "metadata": {
        "id": "2Toxk8_Q0kw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# % correct over all *runs* (unmutated candidate genotype only), for the first and last block (both on the test task).\n",
        "\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "bla = np.loadtxt('blosses0_1stLastB_allruns.txt')\n",
        "print(bla.shape)\n",
        "nbruns = bla.shape[0]//2\n",
        "\n",
        "plt.subplot(2,1,1);\n",
        "mm = np.mean(bla[0::2, :]>0, axis=0)\n",
        "plt.plot(mm, 'g')\n",
        "plt.plot(np.arange(mm.size), .5*np.ones_like(mm), 'k:')\n",
        "plt.ylim([-.1, 1.1])\n",
        "#mm = np.mean(bla[1::2, ::2]>0, axis=0)\n",
        "#plt.plot(np.arange(0,400,2), mm, 'g')\n",
        "#plt.plot(smooth(mm), 'k')\n",
        "plt.ylabel(\"% Correct (over \"+str(nbruns)+\" runs)\")\n",
        "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
        "plt.twinx()\n",
        "plt.gca().get_yaxis().set_ticks([])\n",
        "plt.ylabel(\"Generation 0\")\n",
        "\n",
        "plt.subplot(2,1,2);\n",
        "mm = np.mean(bla[1::2, :]>0, axis=0)\n",
        "plt.plot(mm, 'g')\n",
        "plt.plot(np.arange(mm.size), .5*np.ones_like(mm), 'k:')\n",
        "plt.ylim([-.1, 1.1])\n",
        "#mm = np.mean(bla[1::2, ::2]>0, axis=0)\n",
        "#plt.plot(np.arange(0,400,2), mm, 'g')\n",
        "#plt.plot(smooth(mm), 'k')\n",
        "plt.ylabel(\"% Correct (over \"+str(nbruns)+\" runs)\")\n",
        "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
        "plt.xlabel(\"Trials\")\n",
        "plt.twinx()\n",
        "plt.gca().get_yaxis().set_ticks([])\n",
        "plt.ylabel(\"Generation \"+str(ss))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('trials'+str(TESTTASK).upper()+'.png', dpi=300)"
      ],
      "metadata": {
        "id": "Pf-Lg4kX3jqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdes-ETR7pUP"
      },
      "outputs": [],
      "source": [
        "print(\"Med/min/max/Half-Nth/0th loss in batch:\", float(torch.median(lifelosses)), float(torch.min(lifelosses)), float(torch.max(lifelosses)),\n",
        "      float(lifelosses[BS//2]), float(lifelosses[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN4idp4fPo7m"
      },
      "outputs": [],
      "source": [
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "print(mytask)\n",
        "for nn in range(6):\n",
        "    print(\"Trial\", nn)\n",
        "    ii, tt = taskdata[nn]\n",
        "    print(ii[0,0, 0::15])  # Stim Input 0 for this trial, timesteps 0 and 15, batch element 0\n",
        "    print(tt[0, :, 0])  # Targets for this trial, timestep 0,  batch element 0\n",
        "\n",
        "for nn in range(6):\n",
        "    print(\"Trial\", nn)\n",
        "    ii, tt = taskdata[nn]\n",
        "    print(ii[1,0, 0::15])  # Stim Input 0 for this trial, timesteps 0 and 15, batch element 0\n",
        "    print(tt[1, :, 0])  # Targets for this trial, timestep 0,  batch element 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yqx5PNAxBeR"
      },
      "outputs": [],
      "source": [
        "# This was code for showing the trajectories of multiple neurons over several trials, but we don't store them ('allresps') any more due to mempry constraints\n",
        "\n",
        "if False:\n",
        "    ar1 = torch.dstack(allresps) # Now allresps has dimensions batchsize x neurons x time\n",
        "\n",
        "    # If the network is not chaotic enough and they all have the same weights, they will produce very similar final outputs independently of initial conditions - because eigenvectors !\n",
        "    ar = ar1.cpu().numpy()\n",
        "    print(ar.shape)\n",
        "    ar[ar>1.5] = 1.5\n",
        "    ar[ar<-1.5] = -1.5\n",
        "\n",
        "    plt.figure(figsize=(20,20))\n",
        "    z = plt.plot(ar[1,:20,-1000:].T)  # 20 first neurons of batch element 1\n",
        "    z = plt.plot(ar[1,-1,-1000:].T, 'r--')  # Last (response) neuron of batch element 1\n",
        "    z = plt.plot(ar[1,-2,-1000:].T, 'b--')  # Last (response) neuron of batch element 1\n",
        "    plt.figure(figsize=(10,10))\n",
        "    z = plt.plot(ar[0,:20,-1000:].T)  # 20 first neurons of batch element 0\n",
        "    z = plt.plot(ar[0,-1,-1000:].T, 'r--')  # Last (response) neuron of batch element 0\n",
        "    z = plt.plot(ar[0,-2,-1000:].T, 'b--')  # Last (response) neuron of batch element 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Vhh4MliN9L"
      },
      "outputs": [],
      "source": [
        "# Show the final evolved w matrix (innate weights), and its column and row averages\n",
        "\n",
        "import matplotlib.pyplot as plt; import numpy as np\n",
        "\n",
        "ww = w.cpu().numpy()\n",
        "print(\"Std W:\", np.std(ww), \"Norm W\", np.sqrt(np.sum(ww**2)))\n",
        "zz = np.random.randn(N,N)  * JINIT / np.sqrt(N) \n",
        "print(\"Std w_init_like:\", np.std(zz), \"norm:\", np.sqrt(np.sum(zz**2)))\n",
        "print(np.min(ww), np.max(ww), np.median(ww))\n",
        "print(np.min(zz), np.max(zz), np.median(zz))\n",
        "\n",
        "plt.matshow(ww)\n",
        "plt.figure()\n",
        "plt.plot(np.mean(ww, axis=0), 'r')\n",
        "plt.figure()\n",
        "plt.plot(np.mean(ww, axis=1), 'b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX3rRLttLeVt"
      },
      "outputs": [],
      "source": [
        "# Show the final evolved alpha matrix (plasticity parameters, capital-Pi in the MML paper)\n",
        "\n",
        "\n",
        "aa = alpha.cpu().numpy()\n",
        "\n",
        "print(np.mean(aa))\n",
        "print(np.min(aa), np.max(aa), np.median(aa))\n",
        "plt.matshow(aa)\n",
        "plt.figure()\n",
        "plt.plot(np.mean(aa, axis=0), 'r')\n",
        "plt.figure()\n",
        "plt.plot(np.mean(aa, axis=1), 'b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIA2mdKqLGbd"
      },
      "outputs": [],
      "source": [
        "# Show the mean plastic weights over the whole batch., and for two elements in the batch (note that this is after the end of the last block)\n",
        "\n",
        "pw = bpw.cpu().numpy()\n",
        "\n",
        "print(pw.shape)\n",
        "plt.matshow(np.mean(pw, axis=0))\n",
        "print(np.mean(np.abs(w.cpu().numpy())), np.mean(np.abs(pw)))\n",
        "print(np.max(np.abs(w.cpu().numpy())), np.max(np.abs(pw)))\n",
        "print(np.min(pw[0,:,:]), np.max(pw[0,:,:]), np.median(pw[0,:,:]))\n",
        "plt.figure()\n",
        "plt.matshow(pw[0,:,:])\n",
        "plt.figure()\n",
        "plt.matshow(pw[1,:,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqSm_WSHP_0M"
      },
      "outputs": [],
      "source": [
        "ww = w.cpu().numpy()\n",
        "pw0 = bpw[0,:,:].cpu().numpy()\n",
        "aa = alpha.cpu().numpy()\n",
        "np.savetxt('w.txt', ww)\n",
        "np.savetxt('pw0.txt', pw0)\n",
        "np.savetxt('alpha.txt', aa)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MML_DMS_BS500_400T_Simplify.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIM0MNcvQ9zsG08rPLSH8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
